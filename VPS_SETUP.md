# ðŸš€ Setup VPS cho Nosara Blue Scraper API

## ðŸ“‹ YÃªu cáº§u VPS:
- **OS**: Ubuntu 20.04+ hoáº·c Debian 11+
- **RAM**: Tá»‘i thiá»ƒu 2GB (khuyáº¿n nghá»‹ 4GB)
- **Storage**: 20GB+
- **CPU**: 2 cores+

## ðŸ”§ BÆ°á»›c 1: Chuáº©n bá»‹ VPS

### 1.1 Táº¡o VPS (DigitalOcean, Vultr, Linode, etc.)
```bash
# SSH vÃ o VPS
ssh root@YOUR_VPS_IP
```

### 1.2 Upload code lÃªn VPS
```bash
# Táº¡o thÆ° má»¥c project
mkdir -p /opt/nosara-scraper
cd /opt/nosara-scraper

# Upload files (tá»« mÃ¡y local)
scp main.py requirements.txt setup_vps.sh root@YOUR_VPS_IP:/opt/nosara-scraper/
```

## ðŸ› ï¸ BÆ°á»›c 2: Cháº¡y Setup Script

```bash
# SSH vÃ o VPS
ssh root@YOUR_VPS_IP

# Cháº¡y setup script
cd /opt/nosara-scraper
chmod +x setup_vps.sh
./setup_vps.sh
```

## âœ… BÆ°á»›c 3: Kiá»ƒm tra API

```bash
# Kiá»ƒm tra service status
sudo systemctl status nosara-scraper

# Xem logs
sudo journalctl -u nosara-scraper -f

# Test API
curl http://YOUR_VPS_IP:8000/
```

## ðŸ”§ BÆ°á»›c 4: Cáº¥u hÃ¬nh n8n

### 4.1 Workflow trong n8n:

#### Node 1: HTTP Request (Trigger Scraping)
```
Method: POST
URL: http://YOUR_VPS_IP:8000/scrape
```

#### Node 2: Wait (Optional)
```
Wait for: 30 seconds
```

#### Node 3: HTTP Request (Check Status)
```
Method: GET
URL: http://YOUR_VPS_IP:8000/status
```

#### Node 4: IF Node (Check if completed)
```
Condition: {{ $json.is_running == false }}
```

#### Node 5: HTTP Request (Get Data)
```
Method: GET
URL: http://YOUR_VPS_IP:8000/data
```

### 4.2 Workflow Logic:
1. **Trigger scraping** â†’ POST `/scrape`
2. **Wait 30s** â†’ Ä‘á»ƒ scraper cháº¡y
3. **Check status** â†’ GET `/status`
4. **If completed** â†’ GET `/data`
5. **Process data** â†’ Xá»­ lÃ½ JSON response

## ðŸ“Š API Endpoints:

### 1. **GET /** - Home
```bash
curl http://YOUR_VPS_IP:8000/
```
**Response:**
```json
{
  "message": "Nosara Blue Classes Scraper API",
  "status": "running",
  "endpoints": {
    "/scrape": "POST - Trigger scraping",
    "/status": "GET - Get scraping status",
    "/data": "GET - Get latest data"
  }
}
```

### 2. **POST /scrape** - Trigger Scraping
```bash
curl -X POST http://YOUR_VPS_IP:8000/scrape
```
**Response:**
```json
{
  "success": true,
  "message": "ÄÃ£ báº¯t Ä‘áº§u scraping",
  "status": {
    "is_running": true,
    "last_run": null,
    "total_classes": 0,
    "error": null
  }
}
```

### 3. **GET /status** - Check Status
```bash
curl http://YOUR_VPS_IP:8000/status
```
**Response:**
```json
{
  "is_running": false,
  "last_run": "2025-01-27T10:30:00",
  "total_classes": 45,
  "error": null
}
```

### 4. **GET /data** - Get Data
```bash
curl http://YOUR_VPS_IP:8000/data
```
**Response:**
```json
{
  "success": true,
  "total_classes": 45,
  "data": [
    {
      "event_date": "2025-01-27",
      "start_time": "09:00",
      "end_time": "10:00",
      "title": "Mat Pilates",
      "instructor": "Laura Murillo",
      "location": "Nosara Blue",
      "source_url": "https://www.nosarablue.com/classes",
      "description": "Mat Pilates - 1 hr - 50 spots available",
      "category": "",
      "tags": ""
    }
  ]
}
```

## ðŸ”§ Quáº£n lÃ½ Service:

```bash
# Start service
sudo systemctl start nosara-scraper

# Stop service
sudo systemctl stop nosara-scraper

# Restart service
sudo systemctl restart nosara-scraper

# Check status
sudo systemctl status nosara-scraper

# View logs
sudo journalctl -u nosara-scraper -f

# View recent logs
sudo journalctl -u nosara-scraper -n 50
```

## ðŸ”’ Báº£o máº­t:

### 1. ThÃªm Authentication (Optional)
```bash
# Táº¡o API key
echo "YOUR_API_KEY" > /opt/nosara-scraper/api_key.txt
```

### 2. Setup Nginx Reverse Proxy (Optional)
```bash
# Install nginx
sudo apt install nginx

# Create nginx config
sudo nano /etc/nginx/sites-available/nosara-scraper
```

```nginx
server {
    listen 80;
    server_name YOUR_DOMAIN.com;

    location / {
        proxy_pass http://localhost:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

```bash
# Enable site
sudo ln -s /etc/nginx/sites-available/nosara-scraper /etc/nginx/sites-enabled/
sudo nginx -t
sudo systemctl restart nginx
```

## ðŸš¨ Troubleshooting:

### 1. Service khÃ´ng start
```bash
# Check logs
sudo journalctl -u nosara-scraper -f

# Check Python path
which python3.11

# Check dependencies
pip list
```

### 2. Playwright lá»—i
```bash
# Reinstall Playwright
playwright install --force chromium

# Check system dependencies
ldd $(which chromium)
```

### 3. Port 8000 bá»‹ block
```bash
# Check firewall
sudo ufw status

# Allow port
sudo ufw allow 8000/tcp
```

## ðŸ“ž Support:
- **Logs**: `sudo journalctl -u nosara-scraper -f`
- **Status**: `sudo systemctl status nosara-scraper`
- **Restart**: `sudo systemctl restart nosara-scraper`
