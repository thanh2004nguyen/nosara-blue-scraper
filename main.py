from playwright.sync_api import sync_playwright
import time
from datetime import datetime, timezone, timedelta
import json
import re
import traceback
from fastapi import FastAPI, HTTPException
from fastapi.responses import JSONResponse
import os

app = FastAPI()

def run_scraper_sync():
    """HÃ m cháº¡y scraper Ä‘á»“ng bá»™ vÃ  tráº£ vá» dá»¯ liá»‡u trá»±c tiáº¿p"""
    try:
        with sync_playwright() as p:
            # Cáº¥u hÃ¬nh browser cho Render Linux environment
            browser = p.chromium.launch(
                headless=True,
                args=[
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-accelerated-2d-canvas',
                    '--no-first-run',
                    '--no-zygote',
                    '--disable-gpu',
                    '--single-process',
                    '--disable-extensions',
                    '--disable-plugins',
                    '--disable-images'
                ]
            )
            page = browser.new_page()
            
            print("ğŸš€ Báº¯t Ä‘áº§u truy cáº­p trang web...")
            page.goto("https://www.nosarablue.com/classes")
            print("âœ… ÄÃ£ truy cáº­p trang web thÃ nh cÃ´ng")
            
            # Láº¥y title Ä‘á»ƒ kiá»ƒm tra
            title = page.title()
            print(f"ğŸ“„ Title: {title}")
            
            # Láº¥y URL hiá»‡n táº¡i
            current_url = page.url
            print(f"ğŸ”— URL: {current_url}")
            
            time.sleep(3)
            print("â³ ÄÃ£ chá» 3 giÃ¢y")
            
            # Kiá»ƒm tra xem cÃ³ calendar khÃ´ng
            calendar_divs = page.locator('div[role="list"]')
            calendar_count = calendar_divs.count()
            print(f"ğŸ” TÃ¬m tháº¥y {calendar_count} calendar divs")
            
            # Láº¥y táº¥t cáº£ text trÃªn trang Ä‘á»ƒ debug
            page_text = page.text_content('body')
            print(f"ğŸ“ Page text length: {len(page_text) if page_text else 0}")
            
            # Kiá»ƒm tra xem cÃ³ button nÃ o khÃ´ng
            all_buttons = page.locator('button')
            button_count = all_buttons.count()
            print(f"ğŸ”˜ TÃ¬m tháº¥y {button_count} buttons")
            
            # Kiá»ƒm tra xem cÃ³ text "Classes" khÃ´ng
            if page_text and "Classes" in page_text:
                print("âœ… TÃ¬m tháº¥y text 'Classes' trÃªn trang")
            else:
                print("âŒ KhÃ´ng tÃ¬m tháº¥y text 'Classes'")
            
            # Thá»­ láº¥y má»™t sá»‘ text máº«u
            if page_text:
                sample_text = page_text[:500]
                print(f"ğŸ“„ Sample text: {sample_text}")
            
            browser.close()
            
            # Tráº£ vá» thÃ´ng tin debug
            return {
                "success": True,
                "debug_info": {
                    "title": title,
                    "url": current_url,
                    "calendar_divs_count": calendar_count,
                    "buttons_count": button_count,
                    "page_text_length": len(page_text) if page_text else 0,
                    "has_classes_text": "Classes" in page_text if page_text else False,
                    "sample_text": page_text[:200] if page_text else None
                },
                "total_days": 0,
                "total_classes": 0,
                "daily_summary": [],
                "data": []
            }
            
    except Exception as e:
        print("âœ— Lá»—i:", e)
        traceback.print_exc()
        return {
            "success": False,
            "error": str(e),
            "debug_info": None,
            "total_days": 0,
            "total_classes": 0,
            "daily_summary": [],
            "data": []
        }

@app.get('/')
def home():
    return JSONResponse({
        "message": "Nosara Blue Classes Scraper API",
        "status": "running",
        "endpoints": {
            "/scrape": "POST - Scrape data and return results directly"
        }
    })

@app.post('/scrape')
def scrape_data():
    """API endpoint duy nháº¥t - cháº¡y scraper vÃ  tráº£ vá» dá»¯ liá»‡u trá»±c tiáº¿p"""
    print("ğŸš€ Báº¯t Ä‘áº§u scraping...")
    result = run_scraper_sync()
    print("âœ… HoÃ n thÃ nh scraping")
    return JSONResponse(result)

if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get('PORT', 8000))
    uvicorn.run(app, host='0.0.0.0', port=port)
