from playwright.sync_api import sync_playwright
import time
from datetime import datetime, timezone, timedelta
import json
import re
import traceback
from flask import Flask, jsonify, request
import threading
import os

app = Flask(__name__)

CLASS_ITEMS_XPATH = "//div[@role='list']//div[@role='listitem']"

# Global variable to store scraping status
scraping_status = {
    "is_running": False,
    "last_run": None,
    "total_classes": 0,
    "error": None
}

def get_cst_date():
    cst = timezone(timedelta(hours=-6))
    return datetime.now(cst).date()

def get_next_day(current_date):
    return current_date + timedelta(days=1)

def parse_time_to_24h(time_str):
    try:
        time_str = time_str.strip().lower()
        pattern = r'(\d{1,2}):(\d{2})\s*(am|pm)'
        match = re.match(pattern, time_str)
        if match:
            hour = int(match.group(1))
            minute = int(match.group(2))
            period = match.group(3)
            if period == 'pm' and hour != 12:
                hour += 12
            elif period == 'am' and hour == 12:
                hour = 0
            return f"{hour:02d}:{minute:02d}"
        else:
            return time_str
    except Exception:
        return time_str

def calculate_end_time(start_time, duration):
    try:
        start_hour, start_minute = map(int, start_time.split(':'))
        duration_str = duration.lower()
        hours = 0
        minutes = 0
        hour_pattern = r'(\d+)\s*hr'
        minute_pattern = r'(\d+)\s*min'
        hour_match = re.search(hour_pattern, duration_str)
        minute_match = re.search(minute_pattern, duration_str)
        if hour_match:
            hours = int(hour_match.group(1))
        if minute_match:
            minutes = int(minute_match.group(1))
        end_minute = start_minute + minutes
        end_hour = start_hour + hours + (end_minute // 60)
        end_minute = end_minute % 60
        return f"{end_hour:02d}:{end_minute:02d}"
    except Exception:
        return start_time

def get_week_range_from_html(html):
    try:
        m = re.search(r'([A-Za-z]{3,9}\s+\d{1,2}\s*-\s*[A-Za-z]{3,9}\s+\d{1,2})', html)
        if m:
            return m.group(1).strip()
    except:
        pass
    return ""

def click_next_week_button(page, wait_timeout=6000):
    try:
        prev_html = page.content()
        prev_range = get_week_range_from_html(prev_html)
        candidates = []
        candidates.extend(page.query_selector_all("button[aria-label], a[aria-label], button[title], a[title], button, a, span"))
        elem_to_click = None
        for el in candidates:
            try:
                text = (el.inner_text() or "").strip()
                aria = (el.get_attribute("aria-label") or "").strip()
                title = (el.get_attribute("title") or "").strip()
                if text in ("‚Ä∫", ">", "¬ª", "‚Üí") or re.search(r'\bnext\b', text, re.I) or re.search(r'\bnext\b', aria, re.I) or re.search(r'\bnext\b', title, re.I):
                    try:
                        if not el.is_visible():
                            continue
                    except:
                        pass
                    disabled = el.get_attribute("disabled") == "true" or el.get_attribute("aria-disabled") == "true"
                    if disabled:
                        continue
                    elem_to_click = el
                    break
            except Exception:
                continue

        if not elem_to_click:
            xpath_candidates = page.locator("xpath=//button[normalize-space(.)='‚Ä∫'] | //a[normalize-space(.)='‚Ä∫'] | //span[normalize-space(.)='‚Ä∫']")
            if xpath_candidates.count() > 0:
                el = xpath_candidates.first
                try:
                    if el.is_visible():
                        elem_to_click = el
                except:
                    elem_to_click = el

        if not elem_to_click:
            return False

        try:
            elem_to_click.click()
        except Exception:
            try:
                page.evaluate("(el) => el.click()", elem_to_click)
            except:
                pass

        deadline = time.time() + (wait_timeout/1000)
        while time.time() < deadline:
            html = page.content()
            new_range = get_week_range_from_html(html)
            if new_range and new_range != prev_range:
                time.sleep(0.6)
                return True
            time.sleep(0.3)
        time.sleep(0.5)
        return False

    except Exception as e:
        print("‚ö†Ô∏è click_next_week_button l·ªói:", e)
        return False

def find_and_click_date(page, target_date, max_next_clicks=8):
    day_text = str(target_date.day)
    attempts = 0
    while attempts <= max_next_clicks:
        btns = page.locator(f"button:has-text('{day_text}'), a:has-text('{day_text}'), span:has-text('{day_text}')")
        count = btns.count()
        if count > 0:
            for i in range(count):
                try:
                    el = btns.nth(i)
                    try:
                        if not el.is_visible():
                            continue
                    except:
                        pass
                    if el.get_attribute("aria-disabled") == "true" or el.get_attribute("disabled") == "true":
                        continue
                    try:
                        el.click()
                    except Exception:
                        page.evaluate("(e) => e.click()", el)
                    time.sleep(0.8)
                    return True
                except Exception:
                    continue
        attempts += 1
        print(f"üîÅ Kh√¥ng th·∫•y ng√†y {day_text} (attempt {attempts}/{max_next_clicks}) ‚Äî th·ª≠ Next Week")
        if not click_next_week_button(page):
            print("‚úó Kh√¥ng th·ªÉ click Next Week (ho·∫∑c tu·∫ßn kh√¥ng ƒë·ªïi).")
            return False
        time.sleep(0.6)
    print(f"‚úó ƒê√£ th·ª≠ {max_next_clicks} l·∫ßn next-week m√† v·∫´n kh√¥ng t√¨m th·∫•y ng√†y {day_text}")
    return False

def wait_for_content_change(page, previous_count, timeout_ms=9000):
    try:
        end = time.time() + (timeout_ms / 1000)
        while time.time() < end:
            try:
                if page.locator('text="No Classes Available"').count() > 0:
                    return True
            except:
                pass
            try:
                new_count = page.locator(CLASS_ITEMS_XPATH).count()
                if new_count != previous_count:
                    return True
            except:
                pass
            page.wait_for_timeout(250)
        return False
    except Exception as e:
        print("‚ö†Ô∏è wait_for_content_change l·ªói:", e)
        return False

def extract_class_info(page, current_date):
    try:
        try:
            if page.locator('text="No Classes Available"').count() > 0:
                return "NO_CLASSES"
        except:
            pass

        class_items = page.locator(CLASS_ITEMS_XPATH)
        total_classes = class_items.count()
        classes_info = []
        for i in range(total_classes):
            try:
                rich_text_xpath = f"({CLASS_ITEMS_XPATH})[{i+1}]//div[@data-testid='richTextElement']"
                rich_text_elements = page.locator(rich_text_xpath)
                if rich_text_elements.count() >= 4:
                    get_text = lambda idx: (rich_text_elements.nth(idx).inner_text().strip() if rich_text_elements.count() > idx else "").strip()
                    time_text = get_text(0)
                    duration_text = get_text(1)
                    class_name = get_text(2)
                    teacher_name = get_text(3)
                    spots_text = get_text(4) if rich_text_elements.count() > 4 else ""
                    start_time_24h = parse_time_to_24h(time_text)
                    end_time_24h = calculate_end_time(start_time_24h, duration_text)
                    class_info = {
                        "event_date": current_date.strftime('%Y-%m-%d'),
                        "start_time": start_time_24h,
                        "end_time": end_time_24h,
                        "title": class_name,
                        "instructor": teacher_name,
                        "location": "Nosara Blue",
                        "source_url": "https://www.nosarablue.com/classes",
                        "description": f"{class_name} - {duration_text} - {spots_text}".strip(" -"),
                        "category": "",
                        "tags": ""
                    }
                    classes_info.append(class_info)
            except Exception:
                continue
        return classes_info
    except Exception as e:
        print("‚ö†Ô∏è extract_class_info l·ªói:", e)
        return []

def save_classes_to_file(classes_info, filename="classes_data.json"):
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(classes_info, f, ensure_ascii=False, indent=2)
        print(f"‚úì ƒê√£ l∆∞u {len(classes_info)} l·ªõp h·ªçc v√†o file: {filename}")
        return True
    except Exception as e:
        print(f"‚úó L·ªói khi l∆∞u file: {e}")
        return False

def run_scraper():
    """H√†m ch·∫°y scraper trong thread ri√™ng"""
    global scraping_status
    
    scraping_status["is_running"] = True
    scraping_status["error"] = None
    
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)  # headless=True cho production
            context = browser.new_context()
            page = context.new_page()
            url = "https://www.nosarablue.com/classes"
            print(f"üåê Truy c·∫≠p: {url}")
            
            try:
                page.goto(url, timeout=60000)
                page.wait_for_load_state('networkidle')
                print("‚úì Trang ƒë√£ load")
                
                current_date = get_cst_date()
                end_date = current_date + timedelta(days=30)
                all_classes_info = []
                
                while current_date <= end_date:
                    print(f"\nüîé X·ª≠ l√Ω ng√†y: {current_date}")
                    try:
                        prev_count = page.locator(CLASS_ITEMS_XPATH).count()
                    except:
                        prev_count = -1
                    
                    found = find_and_click_date(page, current_date, max_next_clicks=8)
                    if not found:
                        print(f"‚ö†Ô∏è B·ªè qua ng√†y {current_date} (kh√¥ng t√¨m th·∫•y tr√™n calendar).")
                        current_date = get_next_day(current_date)
                        continue
                    
                    changed = wait_for_content_change(page, prev_count, timeout_ms=9000)
                    if not changed:
                        print("‚ö†Ô∏è N·ªôi dung c√≥ th·ªÉ ch∆∞a c·∫≠p nh·∫≠t, v·∫´n th·ª≠ extract.")
                    
                    classes_info = extract_class_info(page, current_date)
                    if classes_info == "NO_CLASSES":
                        print(f"‚ú≥Ô∏è Ng√†y {current_date}: Kh√¥ng c√≥ l·ªõp.")
                    elif isinstance(classes_info, list) and classes_info:
                        print(f"‚úì Ng√†y {current_date}: t√¨m th·∫•y {len(classes_info)} l·ªõp.")
                        all_classes_info.extend(classes_info)
                    else:
                        print(f"‚ÑπÔ∏è Ng√†y {current_date}: kh√¥ng l·∫•y ƒë∆∞·ª£c l·ªõp (r·ªóng).")
                    
                    current_date = get_next_day(current_date)
                    time.sleep(0.3)
                
                if all_classes_info:
                    save_classes_to_file(all_classes_info)
                    scraping_status["total_classes"] = len(all_classes_info)
                else:
                    print("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu thu th·∫≠p ƒë∆∞·ª£c.")
                    scraping_status["total_classes"] = 0
                
                scraping_status["last_run"] = datetime.now().isoformat()
                
            except Exception as e:
                print("‚úó L·ªói ch·∫°y:", e)
                scraping_status["error"] = str(e)
            finally:
                browser.close()
                print("üîö ƒê√≥ng browser.")
                
    except Exception as e:
        print("‚úó L·ªói kh·ªüi t·∫°o playwright:", e)
        scraping_status["error"] = str(e)
    finally:
        scraping_status["is_running"] = False

@app.route('/')
def home():
    return jsonify({
        "message": "Nosara Blue Classes Scraper API",
        "status": "running",
        "endpoints": {
            "/scrape": "POST - Trigger scraping",
            "/status": "GET - Get scraping status",
            "/data": "GET - Get latest data"
        }
    })

@app.route('/scrape', methods=['POST'])
def trigger_scrape():
    """API endpoint ƒë·ªÉ n8n g·ªçi v√† trigger scraping"""
    global scraping_status
    
    if scraping_status["is_running"]:
        return jsonify({
            "success": False,
            "message": "Scraping ƒëang ch·∫°y, vui l√≤ng ƒë·ª£i",
            "status": scraping_status
        }), 409
    
    # Ch·∫°y scraper trong thread ri√™ng
    thread = threading.Thread(target=run_scraper)
    thread.daemon = True
    thread.start()
    
    return jsonify({
        "success": True,
        "message": "ƒê√£ b·∫Øt ƒë·∫ßu scraping",
        "status": scraping_status
    })

@app.route('/status', methods=['GET'])
def get_status():
    """API endpoint ƒë·ªÉ ki·ªÉm tra tr·∫°ng th√°i scraping"""
    return jsonify(scraping_status)

@app.route('/data', methods=['GET'])
def get_data():
    """API endpoint ƒë·ªÉ l·∫•y d·ªØ li·ªáu m·ªõi nh·∫•t"""
    try:
        if os.path.exists('classes_data.json'):
            with open('classes_data.json', 'r', encoding='utf-8') as f:
                data = json.load(f)
            return jsonify({
                "success": True,
                "total_classes": len(data),
                "data": data
            })
        else:
            return jsonify({
                "success": False,
                "message": "Ch∆∞a c√≥ d·ªØ li·ªáu"
            }), 404
    except Exception as e:
        return jsonify({
            "success": False,
            "message": f"L·ªói ƒë·ªçc d·ªØ li·ªáu: {str(e)}"
        }), 500

if __name__ == "__main__":
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port, debug=False)
